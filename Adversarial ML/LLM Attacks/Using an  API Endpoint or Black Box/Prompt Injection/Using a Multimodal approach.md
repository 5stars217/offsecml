---
tags:
  - llm-attacks
  - prompt-injection
  - black-box
  - multimodal
  - Visual-Attack
transferable: "True"
aliases: 
last_tested: 10/2023
---

## **PoC**
https://twitter.com/fabianstelzer/status/1712790589853352436 
https://twitter.com/rharang/status/1712798576202371217
![[Pasted image 20240116190428.png]]
## **Details**

GPT4v behavior: if instructions in an image clash with the user prompt, it seems to prefer to follow the instructions provided in the image. 