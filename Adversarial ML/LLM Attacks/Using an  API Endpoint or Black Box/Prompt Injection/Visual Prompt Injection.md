---
tags:
  - llm-attacks
  - prompt-injection
  - black-box
transferable: "True"
aliases: 
last_tested: 12/2023
---

## **PoC**
detailed [here](https://www.lakera.ai/blog/visual-prompt-injections) by [Daniel Timbrell](https://www.lakera.ai/author/daniel-timbrell) 

## **Details**

**Visual prompt injection**Â refers to the technique where malicious instructions are embedded within an image. When a model with image processing capabilities, such as GPT-V4, is asked to interpret or describe that image, it might act on those embedded instructions in unintended ways.[1] 

--
[1](https://www.lakera.ai/blog/visual-prompt-injections0) 
