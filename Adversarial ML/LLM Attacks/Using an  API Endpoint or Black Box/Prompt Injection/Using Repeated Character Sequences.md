---
aliases:
  - prompt injection
  - llm attacks
tags:
  - llm-attacks
  - prompt-injection
  - black-box
transferable: "True"
last_tested: 10/2023
---
**PoC**
https://github.com/dropbox/llm-security
by [@mlbri3t](https://twitter.com/mlbr3it)
 
**Details**
Attacks against large language models using repeated character sequences. These techniques can be used to execute prompt injection on content-constrained LLM queries. Used directly or indirectly. 

Tested and confirmed transferable on GPT 3.5T, GPT4, Bard, Llama2.

[paper](https://dropbox.tech/machine-learning/prompt-injection-with-control-characters-openai-chatgpt-llm)